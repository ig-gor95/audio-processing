{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39b92c-515b-4e84-8a63-4e31a5341de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyannote.audio\n",
    "!pip install openai-whisper\n",
    "!pip install torchaudio\n",
    "!pip install -q torch\n",
    "!pip install python-Levenshtein\n",
    "!pip install pydu\n",
    "!pip install speechbrain\n",
    "!pip install torchaudio\n",
    "!pip install silero-vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef32018-e77d-4482-ba8f-57108054440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "/var/folders/nk/wyw3kdr54zq4qvkf61lzbdth0000gn/T/ipykernel_93938/1260678371.py:29: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import EncoderClassifier\n",
      "INFO:datasets:PyTorch version 2.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.stats.proportion as proportion\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as snswsadx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "import torchaudio\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import colorlog\n",
    "import warnings\n",
    "import logging\n",
    "import whisper\n",
    "import timeit\n",
    "import torch\n",
    "import spacy\n",
    "import time\n",
    "import sys\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "import re\n",
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from scipy.stats import ttest_ind,mannwhitneyu,shapiro,norm\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "from IPython.display import display, HTML\n",
    "from datetime import datetime, date, time\n",
    "from sqlalchemy import create_engine\n",
    "from scipy.stats import skew, mode\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm_notebook\n",
    "from itertools import groupby\n",
    "from datasets import Dataset\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "396ef671-4c75-44e9-8a43-4d012f492177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n",
      "MPS (Metal GPU) available: True\n",
      "MPS device name: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS (Metal GPU) available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"MPS device name: {torch.device('mps')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4138861d-4b18-4bb0-a1fb-74c04c4b4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"test_audio.mp3\"\n",
    "BERT_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "TRANSCRIBE_MODEL_NAME = 'medium'\n",
    "\n",
    "id_to_label = {0: 'DOC', 1: 'MDT', 2: 'NAME', 3: 'O', 4: 'ORG', 5: 'POS', 6: 'TEL', 7: 'VOL'}\n",
    "dict_names = {'ORG': 'Организация', 'VOL': 'Объем обработки документов', 'NAME': 'Имя', 'TEL': 'Телефон'}\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"FP16 is not supported on CPU\")\n",
    "logging.getLogger(\"speechbrain\").setLevel(logging.WARNING)\n",
    "LOGGER_LEVEL = logging.DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb013a19-6a7c-4c28-b21d-b32e6442293c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 14:38:01 - INFO     - Logging configured\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def setup_logger():\n",
    "    logger = colorlog.getLogger(__name__)\n",
    "    \n",
    "    logger.handlers = []\n",
    "    logger.propagate = False\n",
    "    \n",
    "    root_logger = logging.getLogger()\n",
    "    if any(isinstance(h, colorlog.StreamHandler) for h in root_logger.handlers):\n",
    "        root_logger.handlers = []\n",
    "    \n",
    "    handler = colorlog.StreamHandler(sys.stdout)\n",
    "    handler.setFormatter(colorlog.ColoredFormatter(\n",
    "        fmt='%(log_color)s%(asctime)s - %(levelname)-8s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        log_colors={\n",
    "            'DEBUG': 'cyan',\n",
    "            'INFO': 'green',\n",
    "            'WARNING': 'yellow',\n",
    "            'ERROR': 'red',\n",
    "            'CRITICAL': 'white,bg_red',\n",
    "        }\n",
    "    ))\n",
    "    \n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(LOGGER_LEVEL)\n",
    "    return logger\n",
    "\n",
    "logging.getLogger().handlers = []\n",
    "logging.root.handlers = []\n",
    "\n",
    "logger = setup_logger()\n",
    "logger.info(\"Logging configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3a8e7c8-7ed7-4ccb-91ff-023c2327b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(file_path):\n",
    "\tlogger.info(f\"{os.path.basename(file_path)} - TRANSCRIBING..\")\n",
    "\tmodel = whisper.load_model(TRANSCRIBE_MODEL_NAME)\n",
    "\treturn model.transcribe(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4cbae97-2730-4213-bc1a-cd221aa1e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7cd0716-98fd-4db0-af1c-0917f4b301b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize(file_path, min_segment_length=1.5):\n",
    "    logger.info(f\"{os.path.basename(file_path)} - DIARIZING..\")\n",
    "    vad_model = load_silero_vad()\n",
    "    embedding_model = EncoderClassifier.from_hparams(\n",
    "        source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "        savedir=\"pretrained_models\"\n",
    "    )\n",
    "    \n",
    "    audio = read_audio(file_path)\n",
    "    speech_timestamps = get_speech_timestamps(audio, vad_model, return_seconds=True)\n",
    "    \n",
    "    valid_segments = []\n",
    "    embeddings = []\n",
    "    \n",
    "    for segment in speech_timestamps:\n",
    "        duration = segment['end'] - segment['start']\n",
    "        if duration < min_segment_length:\n",
    "            continue\n",
    "            \n",
    "        start = int(segment['start'] * 16000)\n",
    "        end = int(segment['end'] * 16000)\n",
    "        segment_wav = audio[start:end]\n",
    "        \n",
    "        if duration < 1.5:\n",
    "            padding = int((1.5 - duration) * 16000)\n",
    "            segment_wav = torch.nn.functional.pad(segment_wav, (0, padding))\n",
    "            \n",
    "        segment_wav = segment_wav.unsqueeze(0)\n",
    "        \n",
    "        try:\n",
    "            embedding = embedding_model.encode_batch(segment_wav)\n",
    "            embeddings.append(embedding.squeeze().numpy())\n",
    "            valid_segments.append(segment)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing segment: {e}\")\n",
    "            continue\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    labels = kmeans.fit_predict(np.array(embeddings))\n",
    "    \n",
    "    return valid_segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daad6ef5-633c-4c00-b456-35d4751e67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_united_result():\n",
    "\tfor view_res in silero_vad_speakers:\n",
    "\t\tlogger.debug(f'Speaker: {view_res[0]} - {view_res[1]}')\n",
    "\t\t\n",
    "def unite_results(transcribed_result, diarized_result, labels, logging = True):\n",
    "\tdiarization_result = []\n",
    "\tbase_string_res = []\n",
    "\tfor i, segment in enumerate(diarized_result):\n",
    "\t\tspeaker = f\"Speaker_{labels[i] + 1}\"\n",
    "\t\tdiarization_result.append({\n",
    "\t\t\t\"start\": segment['start'],\n",
    "\t\t\t\"end\": segment['end'],\n",
    "\t\t\t\"speaker\": speaker\n",
    "\t\t})\n",
    "\tsilero_vad_speakers = []\n",
    "\tfor segment in transcribed_result[\"segments\"]:\n",
    "\t\t\tstart = segment[\"start\"]\n",
    "\t\t\tend = segment[\"end\"]\n",
    "\t\t\ttext = segment[\"text\"]\n",
    "\t\t\tmax_overlap = 0\n",
    "\t\t\tbest_speaker = None\n",
    "\t\n",
    "\t\t\tfor diarization_segment in diarization_result:\n",
    "\t\t\t\toverlap_start = max(start, diarization_segment[\"start\"])\n",
    "\t\t\t\toverlap_end = min(end, diarization_segment[\"end\"])\n",
    "\t\t\t\toverlap = max(0, overlap_end - overlap_start)\n",
    "\t\n",
    "\t\t\t\tif overlap > max_overlap:\n",
    "\t\t\t\t\tmax_overlap = overlap\n",
    "\t\t\t\t\tbest_speaker = diarization_segment[\"speaker\"]\n",
    "\t\n",
    "\t\t\tif best_speaker is None:\n",
    "\t\t\t\tfor diarization_segment in diarization_result:\n",
    "\t\t\t\t\tif diarization_segment[\"end\"] >= start or diarization_segment[\"start\"] <= end:\n",
    "\t\t\t\t\t\tbest_speaker = diarization_segment[\"speaker\"]\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\n",
    "\t\t\tspeaker = best_speaker if best_speaker else \"Unknown\"\n",
    "\t\t\tsilero_vad_speakers.append((speaker, text))\n",
    "\t\t\tbase_string_res.append(text)\n",
    "\treturn base_string_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a27c23-e680-44e8-b18a-7a43e0b978ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_result(results):\n",
    "\treadable_result = []\n",
    "\tfor pred in results:\n",
    "\t\tlabel = id_to_label[int(pred[\"entity\"].split(\"_\")[1])]\n",
    "\t\treadable_result.append({\n",
    "\t\t\t\"word\": pred[\"word\"],\n",
    "\t\t\t\"entity\": label,\n",
    "\t\t\t\"score\": pred[\"score\"],\n",
    "\t\t\t\"start\": pred[\"start\"],\n",
    "\t\t\t\"end\": pred[\"end\"]\n",
    "\t\t})\n",
    "\tcurrent_group = None\n",
    "\tresult = {}\n",
    "\tcurrent_phrase = ''\n",
    "\tdef add_pair(key, value):\n",
    "\t\tif key not in result:\n",
    "\t\t\tresult[key] = []\n",
    "\t\tresult[key].append(value)\n",
    "\tfor res in readable_result:\n",
    "\t\tword = res['word']\n",
    "\t\tgroup = res['entity']\n",
    "\t\tif group == \"O\":\n",
    "\t\t\tif current_group != None:\n",
    "\t\t\t\tadd_pair(current_group, current_phrase)\n",
    "\t\t\tcurrent_group = None\n",
    "\t\t\tcurrent_phrase = ''\n",
    "\t\t\tcontinue\n",
    "\t\tif word.startswith('##'):\n",
    "\t\t\tcurrent_phrase = current_phrase + word.replace('##', '')\n",
    "\t\telif group == current_group:\n",
    "\t\t\tcurrent_phrase = current_phrase + ' ' + word\n",
    "\t\telse:\n",
    "\t\t\tif current_group != None:\n",
    "\t\t\t\tadd_pair(current_group, current_phrase)\n",
    "\t\t\tcurrent_group = group\n",
    "\t\t\tcurrent_phrase = word\n",
    "\tfor key in result:\n",
    "\t    result[key] = list(set(result[key]))\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1beb5e20-ccb7-4480-bc2b-5c7c76d81b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "def processFile(file_name):\n",
    "\tfile_path = os.path.join(os.path.expanduser(\"~\"), file_name)\n",
    "\t\n",
    "\ttranscribed_result = transcribe(file_path)\n",
    "\t\n",
    "\tdiarized_result, labels = diarize(file_path)\n",
    "\t\n",
    "\tprocessed_results = unite_results(transcribed_result, diarized_result, labels)\n",
    "\t\n",
    "\tner_pipeline = pipeline(\"ner\", model=\"./ner-model-2.0\", tokenizer=tokenizer, device=device)\n",
    "\n",
    "\tresults = []\n",
    "\tfor processed_result in processed_results:\n",
    "\t\tparsed = parse_result(ner_pipeline(processed_result))\n",
    "\t\tif len(parsed) > 0:\n",
    "\t\t\tresults.append(parsed)\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "735e567b-c733-4f5b-924e-d48827e6281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 14:39:21 - INFO     - Processing: Разговор-14.mp3\u001b[0m\n",
      "\u001b[32m2025-04-05 14:39:21 - INFO     - Processing: Разговор-13.mp3\u001b[0m\n",
      "\u001b[32m2025-04-05 14:39:21 - INFO     - Processing: Разговор-12.mp3\u001b[0m\n",
      "\u001b[32m2025-04-05 14:39:21 - INFO     - Разговор-14.mp3 - TRANSCRIBING..\u001b[0m\n",
      "\u001b[32m2025-04-05 14:39:21 - INFO     - Разговор-13.mp3 - TRANSCRIBING..\u001b[0m\n",
      "\u001b[32m2025-04-05 14:39:21 - INFO     - Разговор-12.mp3 - TRANSCRIBING..\u001b[0m\n",
      "\u001b[32m2025-04-05 14:39:55 - INFO     - Разговор-14.mp3 - DIARIZING..\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 14:40:01 - INFO     - Разговор-12.mp3 - DIARIZING..\u001b[0m\n",
      "\u001b[36m2025-04-05 14:40:04 - DEBUG    - Разговор-14.mp3 - {'NAME': ['Марина Сергеевна']}\u001b[0m\n",
      "\u001b[36m2025-04-05 14:40:04 - DEBUG    - Разговор-14.mp3 - {'VOL': ['450 документов']}\u001b[0m\n",
      "\u001b[36m2025-04-05 14:40:04 - DEBUG    - Разговор-14.mp3 - {'NAME': ['Антон', 'Марина Сергеев'], 'ORG': ['SchoolD']}\u001b[0m\n",
      "\u001b[36m2025-04-05 14:40:04 - DEBUG    - Разговор-14.mp3 - {'VOL': ['50 документов', '600 документов']}\u001b[0m\n",
      "\u001b[36m2025-04-05 14:40:04 - DEBUG    - Разговор-14.mp3 - {'TEL': ['8983 572 92 31']}\u001b[0m\n",
      "\u001b[32m2025-04-05 14:40:06 - INFO     - Разговор-13.mp3 - DIARIZING..\u001b[0m\n",
      "\u001b[36m2025-04-05 14:40:08 - DEBUG    - Разговор-12.mp3 - {'NAME': ['Анна'], 'ORG': ['Вокруг света']}\u001b[0m\n",
      "\u001b[36m2025-04-05 14:40:08 - DEBUG    - Разговор-12.mp3 - {'VOL': ['300', '200']}\u001b[0m\n",
      "\u001b[36m2025-04-05 14:40:08 - DEBUG    - Разговор-12.mp3 - {'NAME': ['Максим', 'Анна']}\u001b[0m\n",
      "\u001b[36m2025-04-05 14:40:08 - DEBUG    - Разговор-12.mp3 - {'VOL': ['20 документов']}\u001b[0m\n",
      "\u001b[36m2025-04-05 14:40:09 - DEBUG    - Разговор-13.mp3 - {'NAME': ['Елена Смирнова'], 'VOL': ['30 - 40 документов']}\u001b[0m\n",
      "\u001b[36m2025-04-05 14:40:09 - DEBUG    - Разговор-13.mp3 - {'NAME': ['Екатерина', 'Елена'], 'ORG': ['BeautyDoc'], 'VOL': ['200 документов в месяц', '20 карт']}\u001b[0m\n",
      "\u001b[36m2025-04-05 14:40:09 - DEBUG    - Разговор-13.mp3 - {'TEL': ['8357453112']}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ПОДАЕМ НА ВХОД 3 АУДИО ФАЙЛА\n",
    "file_paths = ['Разговор-14.mp3', 'Разговор-13.mp3', 'Разговор-12.mp3',]\n",
    "results_to_print = []\n",
    "    \n",
    "def process_file(file_name):\n",
    "    logger.info(f'Processing: {file_name}')\n",
    "    results = processFile(file_name)\n",
    "    for res in results:\n",
    "        logger.debug(f'{file_name} - {res}')\n",
    "    results_to_print.append((results, file_name))\n",
    "    return results\n",
    "\n",
    "# ЗАПУСКАЕМ ОБРАБОТКУ ФАЙЛОВ\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    executor.map(process_file, file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "263abe7a-4145-4d5c-90e5-1564b2fbf085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл Разговор-14.mp3:\n",
      "Имя:\n",
      "  Марина Сергеевна\n",
      "  Антон\n",
      "  Марина Сергеев\n",
      "Объем обработки документов:\n",
      "  50 документов\n",
      "  450 документов\n",
      "  600 документов\n",
      "Организация:\n",
      "  SchoolD\n",
      "Телефон:\n",
      "  8983 572 92 31\n",
      "\n",
      "Файл Разговор-12.mp3:\n",
      "Имя:\n",
      "  Максим\n",
      "  Анна\n",
      "Организация:\n",
      "  Вокруг света\n",
      "Объем обработки документов:\n",
      "  200\n",
      "  300\n",
      "  20 документов\n",
      "\n",
      "Файл Разговор-13.mp3:\n",
      "Имя:\n",
      "  Екатерина\n",
      "  Елена Смирнова\n",
      "  Елена\n",
      "Объем обработки документов:\n",
      "  200 документов в месяц\n",
      "  30 - 40 документов\n",
      "  20 карт\n",
      "Организация:\n",
      "  BeautyDoc\n",
      "Телефон:\n",
      "  8357453112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ВЫВОД ИЗВЛЕЧЕННЫХ СУЩНОСТЕЙ\n",
    "for result in results_to_print:\n",
    "\tres_dict = {}\n",
    "\tfor data in result[0]:\n",
    "\t\tfor key, value in data.items():\n",
    "\t\t\tif key not in res_dict:\n",
    "\t\t\t\tres_dict[key] = set()\n",
    "\t\t\tfor val in value:\n",
    "\t\t\t\tres_dict[key].add(val)\n",
    "\tprint(f'Файл {result[1]}:')\n",
    "\tfor key, value in res_dict.items():\n",
    "\t\tprint(f'{dict_names[key]}:')\n",
    "\t\tfor val in value:\n",
    "\t\t\tprint(f'  {val}')\n",
    "\tprint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
