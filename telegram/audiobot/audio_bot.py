# -*- coding: utf-8 -*-
# –¢–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç: ASR (Whisper) + –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è (pyannote.audio 3.x) + –∞–∫–∫—É—Ä–∞—Ç–Ω–æ–µ —Ä–∞–∑–Ω–µ—Å–µ–Ω–∏–µ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
import sys, locale

import torch

try:
    locale.setlocale(locale.LC_ALL, "C.UTF-8")
except Exception:
    pass

import asyncio
import os
import tempfile
import subprocess
from dataclasses import dataclass
from typing import List, Optional, Tuple

from pydub import AudioSegment
from openai import OpenAI
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes

# ====== CONFIG ======
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "8146216531:AAGkvbhLkI65FkKaoiVH_rvzgqdknq64M_Q")

# Whisper
OPENAI_STT_MODEL = "whisper-1"
OPENAI_STT_FALLBACK = "gpt-4o-mini-transcribe"

# PyAnnote
HUGGINGFACE_TOKEN = ''
PYANNOTE_PIPE = os.getenv("PYANNOTE_PIPE", "pyannote/speaker-diarization-3.1")  # —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω

# I/O limits
MAX_FILE_MB = 45
MAX_AUDIO_DURATION = 600  # —Å–µ–∫

# ====== TUNING ======
# –û—Ç—Å–µ—á–µ–Ω–∏–µ ¬´–º–∏–∫—Ä–æ¬ª –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ —É –Ω—É–ª–µ–≤–æ–π —Å–µ–∫—É–Ω–¥—ã/–≤–æ–æ–±—â–µ
START_SNAP = 0.15          # –Ω–µ –¥–æ–ø—É—Å–∫–∞–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –±–ª–∏–∂–µ –∫ –Ω—É–ª—é
MIN_DIAR_SPAN = 0.30       # –≤—ã–∫–∏–¥—ã–≤–∞–µ–º —Å–ø–∞–Ω—ã –∫–æ—Ä–æ—á–µ —ç—Ç–æ–≥–æ
DOMINANT_FRAC = 0.85       # –µ—Å–ª–∏ –æ–¥–∏–Ω —Å–ø–∏–∫–µ—Ä –ø–æ–∫—Ä—ã–≤–∞–µ—Ç >= 85% ASR-—Å–µ–≥–º–µ–Ω—Ç–∞ ‚Äî –Ω–∞–∑–Ω–∞—á–∞–µ–º –µ–≥–æ –±–µ–∑ —Ä–µ–∑–∫–∏
MERGE_GAP = 0.35           # —Å–∫–ª–µ–π–∫–∞ –±–ª–∏–∑–∫–∏—Ö –∫—É—Å–∫–æ–≤ –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ –ø—Ä–∏ –≤—ã–≤–æ–¥–µ
MIN_SEG_DUR = 0.35         # –Ω–µ –≤—ã–≤–æ–¥–∏–º —Å–æ–≤—Å–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –≤ —Ä–µ–Ω–¥–µ—Ä–µ
HELLO_FIX_WINDOW = 12.0    # –æ–∫–Ω–æ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π
HELLO_FORCE_CHANGE_T = 3.0 # —Ä–∞–Ω–Ω—è—è —É—Å—Ç–æ–π—á–∏–≤–∞—è —Å–º–µ–Ω–∞ ‚Üí —Ä–µ–∑–∫–∞ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è

def log(s: str):
    try:
        print(str(s))
    except Exception:
        sys.stdout.buffer.write((str(s) + "\n").encode("utf-8", errors="ignore"))

@dataclass
class Segment:
    start: float
    end: float
    text: str
    speaker: Optional[str] = None

def _pick_device() -> torch.device:
    # CUDA > MPS > CPU
    if torch.cuda.is_available():
        return torch.device("cuda")
    if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return torch.device("mps")      # Apple Silicon
    return torch.device("cpu")

# ====== utils ======
def ensure_ffmpeg():
    try:
        subprocess.run(["ffmpeg", "-version"], capture_output=True, text=True, check=True)
    except Exception:
        raise RuntimeError("ffmpeg –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏ ffmpeg –∏ –¥–æ–±–∞–≤—å –≤ PATH.")

def convert_to_wav16k_mono(src_path: str) -> str:
    audio = AudioSegment.from_file(src_path)
    dur = len(audio) / 1000.0
    if dur > MAX_AUDIO_DURATION:
        audio = audio[:MAX_AUDIO_DURATION * 1000]
        log(f"–ê—É–¥–∏–æ –æ–±—Ä–µ–∑–∞–Ω–æ –¥–æ {MAX_AUDIO_DURATION} —Å–µ–∫")
    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)
    out = src_path.rsplit(".", 1)[0] + "_16k.wav"
    audio.export(out, format="wav", parameters=["-ac", "1", "-ar", "16000"])
    return out

def mb(n: int) -> float:
    return n / (1024 * 1024)

# ====== ASR ======
def openai_transcribe(client: OpenAI, wav_path: str) -> Tuple[str, List[Segment]]:
    text = ""
    segments: List[Segment] = []

    def _get(obj, key, default=None):
        if isinstance(obj, dict):
            return obj.get(key, default)
        return getattr(obj, key, default)

    with open(wav_path, "rb") as f:
        try:
            log("üé§ ASR: whisper-1 (verbose_json)")
            resp = client.audio.transcriptions.create(
                model=OPENAI_STT_MODEL,
                file=f,
                response_format="verbose_json",
                temperature=0,
            )
            text = _get(resp, "text", "") or ""
            for s in _get(resp, "segments", []) or []:
                seg_text = (_get(s, "text", "") or "").strip()
                if seg_text:
                    st = float(_get(s, "start", 0.0))
                    en = float(_get(s, "end", 0.0))
                    if en <= st: en = st + 0.10
                    segments.append(Segment(st, en, seg_text))
        except Exception as e:
            log(f"‚ö†Ô∏è whisper-1 —É–ø–∞–ª: {e}; –ø—Ä–æ–±—É–µ–º gpt-4o-mini-transcribe")
            f.seek(0)
            try:
                resp = client.audio.transcriptions.create(
                    model=OPENAI_STT_FALLBACK,
                    file=f,
                    response_format="text",
                    temperature=0,
                )
                text = str(resp) or ""
            except Exception as e2:
                log(f"‚ùå ASR –Ω–µ —É–¥–∞–ª—Å—è: {e2}")
                return "", []

    if not segments and text:
        try:
            import librosa
            dur = librosa.get_duration(path=wav_path)
        except Exception:
            dur = 0.01
        segments = [Segment(0.0, max(dur, 0.10), text)]
    log(f"‚úÖ ASR: {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
    return text, segments

# ====== PyAnnote diarization ======
def diarize_with_pyannote(wav_path: str) -> List[Tuple[float, float, str]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç [(start, end, 'SPEAKER_00'), ...] ‚Üí –ø–æ–∑–∂–µ –ø–µ—Ä–µ–ª–µ–π–±–ª–∏–º –≤ ¬´–°–ø–∏–∫–µ—Ä 1/2¬ª.
    """
    try:
        import torch
        from pyannote.audio import Pipeline
    except Exception as e:
        log(f"‚ÑπÔ∏è PyAnnote –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        return []

    if not HUGGINGFACE_TOKEN:
        log("‚ÑπÔ∏è –ù–µ –∑–∞–¥–∞–Ω HUGGINGFACE_TOKEN ‚Äî –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
        return []

    device = _pick_device()
    pipeline = Pipeline.from_pretrained(
        "pyannote/speaker-diarization-3.1",
        use_auth_token=HUGGINGFACE_TOKEN,
    )
    pipeline.to(device)

    try:
        diar = pipeline(wav_path)
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞ PyAnnote: {e}")
        return []

    # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
    spans: List[Tuple[float, float, str]] = []
    for turn, _, speaker in diar.itertracks(yield_label=True):
        s, e = float(turn.start), float(turn.end)
        if s is None or e is None:
            continue
        s = max(START_SNAP, s)  # –Ω–µ –ø—É—Å–∫–∞–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –∫ 0.00
        if e - s >= MIN_DIAR_SPAN:
            spans.append((s, e, str(speaker)))
    spans.sort(key=lambda x: (x[0], x[1]))
    log(f"üîä PyAnnote: –ø–æ–ª—É—á–µ–Ω–æ {len(spans)} –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤")
    return spans

# ====== –ú–µ—Ç–∫–∏ ¬´–°–ø–∏–∫–µ—Ä 1/2¬ª –ø–æ –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –≤ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–∏ ======
def relabel_speakers(diar: List[Tuple[float, float, str]]) -> List[Tuple[float, float, str]]:
    if not diar: return diar
    # –Ω–∞–∫–æ–ø–∏–º –≤—Ä–µ–º—è –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º –≤ –ø–µ—Ä–≤—ã—Ö —Å–µ–∫—É–Ω–¥–∞—Ö
    votes = {}
    for s, e, lab in diar:
        if s >= HELLO_FIX_WINDOW: break
        ov = min(e, HELLO_FIX_WINDOW) - max(s, 0.0)
        if ov > 0:
            votes[lab] = votes.get(lab, 0.0) + ov
    if not votes:
        return diar
    first_label = max(votes.items(), key=lambda x: x[1])[0]
    others = [lab for _, _, lab in diar if lab != first_label]
    second_label = others[0] if others else None
    mapping = {first_label: "–°–ø–∏–∫–µ—Ä 1"}
    if second_label:
        mapping[second_label] = "–°–ø–∏–∫–µ—Ä 2"
    # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 2 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ‚Äî –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—É—Å—Ç—å –∏–¥—É—Ç –∫–∞–∫ –µ—Å—Ç—å, –Ω–æ –æ–±—ã—á–Ω–æ –∏—Ö 2
    return [(s, e, mapping.get(lab, lab)) for s, e, lab in diar]

# ====== –ú—ç–ø–ø–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ø–∏–∫–µ—Ä–æ–≤ ======
def split_text_by_overlap(asr_segments: List[Segment],
                          diar_spans: List[Tuple[float, float, str]]) -> List[Segment]:
    if not diar_spans:
        return asr_segments

    diar_spans = relabel_speakers(diar_spans)

    def dominant_speaker(a: float, b: float) -> str:
        votes = {}
        for s, e, lab in diar_spans:
            if e <= a or s >= b:
                continue
            ov = min(b, e) - max(a, s)
            if ov > 0:
                votes[lab] = votes.get(lab, 0.0) + ov
        if not votes:
            return "–°–ø–∏–∫–µ—Ä 1"
        return max(votes.items(), key=lambda x: x[1])[0]

    result: List[Segment] = []
    for seg in asr_segments:
        st, en = seg.start, seg.end
        if en <= st or not seg.text.strip():
            continue

        # –í –æ–∫–Ω–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è –∂—ë—Å—Ç—á–µ: –µ—Å–ª–∏ —Ä–∞–Ω–Ω—è—è —Å–º–µ–Ω–∞ ‚Äî –±–µ—Ä—ë–º –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –±–µ–∑ –ª–∏—à–Ω–µ–π —Ä–µ–∑–∫–∏
        if st < HELLO_FORCE_CHANGE_T:
            lab = dominant_speaker(st, min(en, HELLO_FIX_WINDOW))
            result.append(Segment(st, en, seg.text, lab))
            continue

        # –ò—â–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è —Å –¥–∏–∞—Ä-–∫—É—Å–∫–∞–º–∏
        overlaps = []
        for ts, te, lab in diar_spans:
            if en <= ts or st >= te:
                continue
            s0, e0 = max(st, ts), min(en, te)
            if e0 - s0 > 0.20:
                overlaps.append((s0, e0, lab))
        if not overlaps:
            lab = dominant_speaker(st, en)
            result.append(Segment(st, en, seg.text, lab))
            continue

        seg_len = en - st
        best = max(overlaps, key=lambda x: x[1]-x[0])
        if (best[1]-best[0]) / seg_len >= DOMINANT_FRAC:
            result.append(Segment(st, en, seg.text, best[2]))
        else:
            lab = dominant_speaker(st, en)
            result.append(Segment(st, en, seg.text, lab))

    # —Å–∫–ª–µ–π–∫–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤
    merged: List[Segment] = []
    for s in sorted(result, key=lambda x: x.start):
        if merged and merged[-1].speaker == s.speaker and s.start - merged[-1].end <= MERGE_GAP:
            merged[-1].end = max(merged[-1].end, s.end)
            merged[-1].text = (merged[-1].text + " " + s.text).strip()
        else:
            merged.append(s)
    return merged

# ====== Rendering ======
def ts(sec: float) -> str:
    m, s = divmod(int(max(0, sec)), 60)
    return f"{m:02d}:{s:02d}"

def render_transcript(segments: List[Segment]) -> str:
    if not segments:
        return "‚ö†Ô∏è –ü—É—Å—Ç–æ"
    lines = []
    cur_spk, cur_s, cur_e, buf = None, None, None, []
    for seg in sorted(segments, key=lambda x: x.start):
        if seg.end - seg.start < MIN_SEG_DUR:
            continue
        spk = seg.speaker or "–°–ø–∏–∫–µ—Ä 1"
        if cur_spk is None:
            cur_spk, cur_s, cur_e, buf = spk, seg.start, seg.end, [seg.text]
            continue
        if spk == cur_spk and seg.start - cur_e <= MERGE_GAP:
            cur_e = max(cur_e, seg.end)
            buf.append(seg.text)
        else:
            lines.append(f"\nüé§ {cur_spk}:\n   [{ts(cur_s)}-{ts(cur_e)}] {' '.join(buf).strip()}")
            cur_spk, cur_s, cur_e, buf = spk, seg.start, seg.end, [seg.text]
    if buf:
        lines.append(f"\nüé§ {cur_spk}:\n   [{ts(cur_s)}-{ts(cur_e)}] {' '.join(buf).strip()}")
    return "\n".join(lines)

def render_stats(segments: List[Segment]) -> str:
    totals, counts = {}, {}
    for s in segments:
        sp = s.speaker or "–°–ø–∏–∫–µ—Ä 1"
        totals[sp] = totals.get(sp, 0.0) + max(0.0, s.end - s.start)
        counts[sp] = counts.get(sp, 0) + 1
    lines = ["\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:"]
    for sp in sorted(totals):
        m, ss = divmod(int(totals[sp]), 60)
        lines.append(f"   {sp}: {m:02d}:{ss:02d} ({counts[sp]} —Ä–µ–ø–ª–∏–∫)")
    return "\n".join(lines)

# ====== Telegram ======
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text(
        f"üéôÔ∏è –ü—Ä–∏—à–ª–∏—Ç–µ –∞—É–¥–∏–æ ‚Äî —Ä–∞—Å—à–∏—Ñ—Ä—É—é –∏ —Ä–∞–∑–Ω–µ—Å—É –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º (PyAnnote + Whisper).\n‚ö° –î–æ {MAX_FILE_MB} –ú–ë –∏–ª–∏ {MAX_AUDIO_DURATION//60} –º–∏–Ω—É—Ç."
    )

async def handle_audio(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    msg = update.message
    log("üîç –ü–æ–ª—É—á–µ–Ω –∞—É–¥–∏–æ—Ñ–∞–π–ª")
    tg_file, file_name = None, "audio"
    if msg.voice:
        tg_file = await context.bot.get_file(msg.voice.file_id); file_name = "voice.ogg"
    elif msg.audio:
        tg_file = await context.bot.get_file(msg.audio.file_id); file_name = msg.audio.file_name or "audio"
    elif msg.document and (msg.document.mime_type or "").startswith("audio/"):
        tg_file = await context.bot.get_file(msg.document.file_id); file_name = msg.document.file_name or "audio"
    else:
        await msg.reply_text("‚ùå –ü—Ä–∏—à–ª–∏—Ç–µ –∞—É–¥–∏–æ."); return

    with tempfile.TemporaryDirectory() as tmpdir:
        src = os.path.join(tmpdir, file_name)
        await tg_file.download_to_drive(src)
        if mb(os.path.getsize(src)) > MAX_FILE_MB:
            await msg.reply_text("‚ùå –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π."); return

        ensure_ffmpeg()
        wav = convert_to_wav16k_mono(src)

        status = await msg.reply_text("üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é...")
        try:
            client = OpenAI(api_key=OPENAI_API_KEY)

            await status.edit_text("üîÑ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é (Whisper)...")
            _, asr_segments = await asyncio.get_event_loop().run_in_executor(None, openai_transcribe, client, wav)
            if not asr_segments:
                await status.edit_text("‚ùå –ü—É—Å—Ç–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è."); return

            await status.edit_text("üîä –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è (PyAnnote)...")
            diar = await asyncio.get_event_loop().run_in_executor(None, diarize_with_pyannote, wav)

            if diar:
                mapped = split_text_by_overlap(asr_segments, diar)
                transcript = render_transcript(mapped)
                sp_count = len({sp for *_, sp in diar})
                head = f"üéôÔ∏è –†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –∞—É–¥–∏–æ\nüéØ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Å–ø–∏–∫–µ—Ä–æ–≤: {sp_count}\n"
                body = head + transcript + render_stats(mapped)
            else:
                # fallback –±–µ–∑ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
                for s in asr_segments: s.speaker = None
                transcript = render_transcript(asr_segments)
                head = "üéôÔ∏è –†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –∞—É–¥–∏–æ\n‚ö†Ô∏è –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (PyAnnote –Ω–µ —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω)\n"
                body = head + transcript + render_stats(asr_segments)

            await status.delete()
            preview = body[:1500] + ("\n\n...(—Å–º. —Ñ–∞–π–ª)" if len(body) > 1500 else "")
            await msg.reply_text(preview)

            import io
            bio = io.BytesIO(body.encode("utf-8")); bio.name = "transcript.txt"
            await msg.reply_document(bio)

        except Exception as e:
            await status.edit_text(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            log(f"–û—à–∏–±–∫–∞: {e}")

def main():
    app = Application.builder().token(BOT_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.VOICE | filters.AUDIO | filters.Document.AUDIO, handle_audio))
    log("‚ö° –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω...")
    app.run_polling()

if __name__ == "__main__":
    main()
